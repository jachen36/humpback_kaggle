{"cells":[{"metadata":{"trusted":true,"_uuid":"5423de34748b7162670bf99af67482c98f7cfddf"},"cell_type":"code","source":"!pip install fastai==1.0.41 -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import fastai\nfrom fastai import *\nfrom fastai.vision import *\nprint(f'fastai version: {fastai.__version__}')\nprint(f'torch version: {torch.__version__}')\n\nverbose = False  # print out extra details?\n\n# import matplotlib.patches.Path\nfrom matplotlib.patches import Rectangle\n%matplotlib inline\n\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9c51e39e2a729ce09ab884b4bb6ee6e76938b3e"},"cell_type":"code","source":"# glogal settings\ndata_fp = Path('../input')\ndata_train = data_fp/'whale-categorization-playground'/'train'/'train'\ncrop_fp = data_fp/'cropping_whale2'/'cropping.txt'  ## From this kernel, https://www.kaggle.com/martinpiotte/bounding-box-model/output\ncrop_coco = data_fp/'cropping-whale-coco'/'coco_whale.json'  # cropping.txt was convert to coco format so that fastai get_annotation can be used\n\nbs = 64\nnum_workers = 0  # set to zero because get error : 'DataLoader worker (pid 56) is killed by signal: Bus error. '\nsz = 224 ## resize images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6ed04b0f1b6eaffad8d6066fdb03bb8ba4a8004"},"cell_type":"markdown","source":"## Create DataBunch with Coco Format"},{"metadata":{"trusted":true,"_uuid":"3eac4a4b2105aca9a6a7540983b8345012e7720a"},"cell_type":"code","source":"images, lbl_bbox = get_annotations(crop_coco)\nimg2bbox = dict(zip(images, lbl_bbox))\nget_y_func = lambda o: img2bbox[Path(o).name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87ccc3828beb45be13e839632f705c70a10e4d96"},"cell_type":"code","source":"tfm = get_transforms(flip_vert=False, \n                     # doesn't make sense to have upside down tails\n                     max_rotate=0.3)\n                     # rotating too much will cause the bbox to be super large and not accurate\nif verbose: tfm  # show the list of transformation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6f1a77252391f77b741fbe1a2c73ae0f84d11ed"},"cell_type":"code","source":"data = (ObjectItemList.from_df(pd.DataFrame(data=images), path=data_train)\n        .random_split_by_pct(seed=52)                          \n        #How to split in train/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func\n        .transform(get_transforms(), \n                   tfm_y= True, \n                   size=sz, \n                   resize_method=ResizeMethod.SQUISH,\n                   padding_mode='border')\n        #Data augmentation? -> Standard transforms with tfm_y=True\n        .databunch(bs=bs, collate_fn=bb_pad_collate, num_workers=num_workers)   \n        #Finally we convert to a DataBunch and we use bb_pad_collate\n        .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ec3efaa8e063b8db55f6a6c47fc0f3e7bfb3e1"},"cell_type":"markdown","source":"### Show how data augmentation on image and the bounding box"},{"metadata":{"trusted":true,"_uuid":"705971abe3bbb7ca16e4f088b4aa9fcbff91e2a8"},"cell_type":"code","source":"idx = 65\nfig, axes = plt.subplots(3,3, figsize=(9,9))\nfor i, ax in enumerate(axes.flat):\n    img = data.train_ds[idx]\n    # image is augmented each time it is retrived\n    img[0].show(y=img[1], ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"134abeb022ff23ab51429d5ebc83439488fe679a"},"cell_type":"code","source":"data.show_batch(rows=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e2a37e646538466c89443a162bcfcb30616ce3c"},"cell_type":"markdown","source":"## Training "},{"metadata":{"trusted":true,"_uuid":"fdeeb563e0a2620df8e0a63c5f7cc9ec15dd3cad"},"cell_type":"code","source":"# L1Loss is used instead of MSE is because MSE penalize mistake more than it should \ndef loss_func(preds, targs, class_idx, **kwargs):\n    return nn.L1Loss()(preds, targs.squeeze())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39bd111c9ee8aaf65aa3506805b3853de8c5b52a"},"cell_type":"code","source":"head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))\nlearn = create_cnn(data=data, arch=models.resnet18, pretrained=True, custom_head=head_reg4,\n                  model_dir = '/tmp/models')\nlearn.loss_func = loss_func","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f471a6a84f72e92cfd3efc2be027eec3ffabe94"},"cell_type":"code","source":"if verbose: print(learn.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"642a1d52fb80a49d7d1933a8695cf443631d9176"},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cadea9a5daa3393c28133ec85cfcd7b0f855ffa"},"cell_type":"code","source":"learn.fit(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b82768b6c12c292acfa4e54e0c6e5ebdf5b7d088"},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63b3c69a79665516d9452e786d5650bc83cd3aff"},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b109a5bac29aede061aa11c5d9abbfd1ad0175c3"},"cell_type":"code","source":"learn.fit(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb297767a7db00f0f5f1c49acd1875ff89179b14"},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80ec28c5f33285face6779b9675e227cbd464534"},"cell_type":"markdown","source":"## Check Result"},{"metadata":{"trusted":true,"_uuid":"fb682dcdb72b040c70f8181a4ba059ff3efe6cc4"},"cell_type":"code","source":"preds, targs = learn.get_preds(ds_type=DatasetType.Valid)\ntargs = targs.squeeze()  # fastai expect multiple objects but we only have 1\n# preds = torch.clamp(preds, -1,1) # making sure the preds values are within the picture","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b617827838ad40bfc909857ee68af52dd3cd609"},"cell_type":"code","source":"np.random.seed(24)\nn = 10  # look at n samples\nidxs = np.random.randint(0,len(data.valid_ds), size=n)\nprint(idxs)\n_, axes = plt.subplots(nrows=n, ncols=2, figsize = (15,20))\nfor i, row in zip(idxs, axes):\n    ## get the img from valid_ds\n    ## get the targs and preds\n    ## also need to get axxes\n    img = data.valid_ds[i][0].data  # image resize after data is called else original image size\n    img_name = Path(data.valid_ds.items[i]).name\n    img_size = img.shape[1:]\n    targ, pred = targs[i], preds[i]\n    print(targ, pred)\n#     pred = torch.tensor([-0.3,-.6,.1,.6])  # For testing\n    for l, v, ax in zip(['Target', 'Prediction'], [targ, pred], row):\n        Image(img).show(ax=ax,\n                         y=ImageBBox.create(*img_size, \n                                            bboxes=v.unsqueeze(0),\n                                            scale=False),\n                        title=l + \":\" + img_name)    \n        \n## It is noticably that the x axis of the bbox is really bad. It is consistently out of range of -1 to 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f408d72de7213613b071729a96ada1acf811b40"},"cell_type":"code","source":"np.random.seed(45)\nn = 10  # look at n samples\nidxs = np.random.randint(0,len(data.valid_ds), size=n)\nprint(idxs)\n_, axes = plt.subplots(nrows=n, ncols=2, figsize = (15,20))\nfor i, row in zip(idxs, axes):\n    ## get the img from valid_ds\n    ## get the targs and preds\n    ## also need to get axxes\n    img = data.valid_ds[i][0].data  # image resize after data is called else original image size\n    img_name = Path(data.valid_ds.items[i]).name\n    img_size = img.shape[1:]\n    targ, pred = targs[i], preds[i]\n    print(targ, pred)\n#     pred = torch.tensor([-0.3,-.6,.1,.6])  # For testing\n    for l, v, ax in zip(['Target', 'Prediction'], [targ, pred], row):\n        Image(img).show(ax=ax,\n                         y=ImageBBox.create(*img_size, \n                                            bboxes=v.unsqueeze(0),\n                                            scale=False),\n                        title=l + \":\" + img_name)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1575e0340cb83754b9d7306ca0c6a7a7710eaca2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb1510f6e0e9f7646056254ddbd25484f28f2f0e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bb142280a90f278521020c4976f7cfd8fd38577"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7f259f0c100aefd19135ec9c7ba911cd2bb9e3e"},"cell_type":"code","source":"pd.DataFrame(data = preds.numpy()).to_csv('testing.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ec906c003bdebaf03978ff6192d18abfd255d4d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff300e7bf6f049eb842ceedb1db53cb3635d2aa9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45279f101eb664c87ff33e31e4f03cc3ecc63ca8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54d56d2fe512527fe3ba8a180d19663671718806"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad5a722cda4ee7c15d7d5d6c41814bf6552db317"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a58d506ef6cb2f6d97f2510a4be85d8cd8612e64"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce83e577d671e9ce047930f18ee86d3b09ef8463"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"141fe892efc2b91d81c69f253d999ef34ab8cb52"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b0279b95c3789a1c785cc3e41c7311a5a7582ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56d7eaa4ea25f93ef9df64ec17261b9a9bdf18ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}