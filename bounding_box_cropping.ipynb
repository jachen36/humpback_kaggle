{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box model\n",
    "\n",
    "The concept and data is from this kaggle kernel: [Bounding Box Model](https://www.kaggle.com/martinpiotte/bounding-box-model)\n",
    "\n",
    "Also from this notebook: [github](https://github.com/radekosmulski/whale/blob/master/fluke_detection_redux.ipynb)\n",
    "\n",
    "The idea is standardize the image focus and to make it easier for the classification model to recognize whale id. The data provided was manually created by placing landmarks on the whale tail and using the maximum value of them to create the the cropping location. \n",
    "\n",
    "There are 1200 bounding box samples. The data is not from this competition but from the [playground](https://www.kaggle.com/c/whale-categorization-playground) \n",
    "\n",
    "Make sure to make to upgrade your fastai. I was using 1.0.38 & 1.0.39 which was giving errors when creating the databunch until it was update to 1.0.41. error: ' can't convert np.ndarray of type numpy.object_.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai version: 1.0.41\n",
      "torch version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "print(f'fastai version: {fastai.__version__}')\n",
    "print(f'torch version: {torch.__version__}')\n",
    "\n",
    "verbose = True  # print out extra details?\n",
    "\n",
    "# import matplotlib.patches.Path\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# to stop fastai from printing out \"UserWarning: Tensor is int32: upgrading to int64;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pynvml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d236184d2872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpynvml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pynvml'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global setting\n",
    "data_fp = Path('data')\n",
    "data_train = data_fp/'train'\n",
    "data_playground = data_fp/'train_playground'\n",
    "data_test = data_fp/'train_playground'  # this should be change to train for the cropping\n",
    "crop_fp = data_fp/'cropping.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "num_workers = 3  # set to zero when using kaggle kernel. It crashing the kernel if not\n",
    "sz = 224 ## resize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into the cropping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(crop_fp, 'rt') as f:\n",
    "    crop_ls = [r.split(',') for r in f.read().split('\\n') if len(r.split(',')) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = [(img, [(int(coords[i]), int(coords[i+1])) for i in range(0, len(coords), 2)]) \n",
    "                                                         for img, *coords in crop_ls]\n",
    "\n",
    "set([len(r[1]) for r in crop_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are varying number of point pairs for each image ranging from 4 to 11 points. To obtain a bounding box, one need to get the min, max value of the x and y axis. \n",
    "\n",
    "Each step converting data into the right format to use\n",
    "```python\n",
    "    text file: \"image_filename, x1, y1, x2, x3, ... \\n .....]\n",
    "    crop_ls: [image_filename, x1, y1, x2, x3, ...]\n",
    "    crop_data: [image_filename, [(x1, y1), (x2, y2), ..]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(coords):\n",
    "    x, y = [x for x,_ in coords], [y for _,y in coords]\n",
    "    xmin, xmax= min(x), max(x)\n",
    "    ymin, ymax = min(y), max(y)\n",
    "    # lower left corner, width and height\n",
    "    return xmin, ymin, xmax-xmin, ymax-ymin\n",
    "\n",
    "def draw_bbox(box):\n",
    "    return Rectangle((box[0], box[1]), box[2], box[3],\n",
    "                     linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "def img_bbox(data):\n",
    "    img = PIL.Image.open(data_playground/data[0])\n",
    "    _, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.scatter([x for x,_ in data[1]], [y for _,y in data[1]], marker='o', c='r')\n",
    "    ax.add_patch(draw_bbox(get_bbox(data[1])))\n",
    "    ax.set_title(data[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox is created from the landmarks\n",
    "img_bbox(crop_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data into Coco dataset format and then to fastai format\n",
    "\n",
    "\n",
    "**Coco format**\n",
    "```json\n",
    "{\n",
    "    \"categories\": [\n",
    "        {\"id\": 0, \"name\": \"whale\"},\n",
    "        {\"id\": 1, \"name\": \"placeholder\"}\n",
    "    ],\n",
    "    \"images\": [\n",
    "        {\"id\": 1000, \"file_name\": \"whale1.jpg\"},\n",
    "        {\"id\": 1001, \"file_name\": \"whale2.jpg\"}\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        {\"image_id\": 1000, \"bbox\": [x, y, width, height], \"category_id\": 0},\n",
    "        {\"image_id\": 1001, \"bbox\": [x, y, width, height], \"category_id\": 0}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Fastai Format** for multiple objects in an image\n",
    "```python\n",
    "[\n",
    "    [image_fn, image_fn],\n",
    "    [[\n",
    "        [[[top, left, bottom, right], \n",
    "          [[top, left, bottom, right]], \n",
    "         ['whale', 'whale']],\n",
    "        [[[[top, left, bottom, right], \n",
    "          [[top, left, bottom, right]], \n",
    "         ['whale', 'whale']]\n",
    "    ]]  \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations = [], []\n",
    "for i, v in enumerate(crop_data, start=1000):\n",
    "    images.append({\"id\": i, \"file_name\": v[0]})\n",
    "    annotations.append({\"image_id\": i, \"bbox\": get_bbox(v[1]), \"category_id\": 0})\n",
    "\n",
    "categories = [{\"id\": 0, \"name\": \"whale\"}]\n",
    "\n",
    "coco_whale = {\"categories\": categories,\n",
    "              \"images\": images,\n",
    "              \"annotations\": annotations}\n",
    "\n",
    "with open(\"data/coco_whale.json\", \"w+\") as f:\n",
    "    json.dump(obj=coco_whale, fp=f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del images, annotations, categories, coco_whale, i, v, crop_data, crop_fp, crop_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing that the conversion was done corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_images, tmp_lbl_bbox = get_annotations('data/coco_whale.json')\n",
    "len(tmp_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coco_dataset format is (x, y, width, height)\n",
    "\n",
    "fastai expect (y_upper_left, x_upper_left, y_lower_right, x_lower_right) with the origin in the upper left hand corner of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(Path('data/train_playground')/tmp_images[0])\n",
    "print(f'BBox coords: {tmp_lbl_bbox[0][0]}')\n",
    "bbox = ImageBBox.create(*img.size, tmp_lbl_bbox[0][0])\n",
    "img.show(y=bbox, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmp_images, tmp_lbl_bbox, img, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataBunch with Coco Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, lbl_bbox = get_annotations('data/coco_whale.json')\n",
    "img2bbox = dict(zip(images, lbl_bbox))\n",
    "get_y_func = lambda o: img2bbox[Path(o).name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose: lbl_bbox[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = get_transforms(flip_vert=False, \n",
    "                     # doesn't make sense to have upside down tails\n",
    "                     max_rotate=0.3,\n",
    "                      # rotating too much will cause the bbox to be super large and not accurate\n",
    "                     max_zoom=1)\n",
    "                     # remove zooming \n",
    "    \n",
    "if verbose: # show the list of transformation\n",
    "    for i in tfm:\n",
    "        for j in i: print(j)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OneObjectCategoryList(ObjectCategoryList):\n",
    "#     def analyze_pred(self, pred): return [pred.unsqueeze(0), torch.zeros(1).long().unsqueeze(0)]\n",
    "# class ObjectItemListOne(ImageItemList):\n",
    "#     _label_cls,_square_show_res = OneObjectCategoryList,False\n",
    "\n",
    "# It might just be because windows is not supported that is why i am getting the [Errno 32] Broken pipe\n",
    "# when ever I do data.show_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ObjectItemList.from_df(pd.DataFrame(data=images), path=data_fp, folder='train_playground')\n",
    "        .random_split_by_pct(seed=52)                          \n",
    "        #How to split in train/valid? -> randomly with the default 20% in valid\n",
    "        .label_from_func(get_y_func)\n",
    "        #How to find the labels? -> use get_y_func\n",
    "        .add_test_folder('test')  # TODO change to actual competition data\n",
    "        .transform(get_transforms(), \n",
    "                   tfm_y= True, \n",
    "                   size=sz, \n",
    "                   resize_method=ResizeMethod.SQUISH,\n",
    "                   padding_mode='border')\n",
    "        #Data augmentation? -> Standard transforms with tfm_y=True\n",
    "        .databunch(bs=bs, collate_fn=bb_pad_collate, num_workers=num_workers)   \n",
    "        #Finally we convert to a DataBunch and we use bb_pad_collate\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.tfm_y = False  # test set has no y value so no transformation for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 65\n",
    "fig, axes = plt.subplots(3,3, figsize=(9,9))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = data.train_ds[idx]\n",
    "    # image is augmented each time it is retrived\n",
    "    img[0].show(y=img[1], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For troubleshooting. Remove when ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.valid_ds.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.y[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for i in data.test_ds.y:\n",
    "    count += 1\n",
    "    if count % 100 == 0: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt with resnet18 with a simple custom head of nn.Sequential(Flatten(), nn.Linear(25088,4)) did not have good result: the xaxis are out of the range [-1,1] and are consistently around -2.8 and 1.5. the y axis is two narrow. I believe the reason is because the bbox for the xaxis is consistently at the edge of the image and so being outside is reasonable.\n",
    "\n",
    "Second attempt:\n",
    "TODO:\n",
    "* Increase the complexity of the custom head with some non-linear features\n",
    "* new loss function\n",
    "* new metrics\n",
    "* more augmentation\n",
    "* Use a larger resnet\n",
    "* increase bs\n",
    "* using  fit_one_cycle instead of fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1Loss is used instead of MSE is because MSE penalize mistake more than it should \n",
    "def loss_func(preds, targs, class_idx, **kwargs):\n",
    "    return nn.L1Loss()(preds, targs.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_reg4 = nn.Sequential(\n",
    "    Flatten(), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(25088,256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 4))\n",
    "    # Maybe add nn.tanh since the values are [-1,1]\n",
    "learn = create_cnn(data=data, arch=models.resnet18, pretrained=True, custom_head=head_reg4,\n",
    "#                    model_dir = '/tmp/models'  ## For kaggle kernel \n",
    "                  )\n",
    "learn.loss_func = loss_func\n",
    "# change the loss function??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose: print(learn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(15, pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(15, max_lr = slice(0.001, 0.001/5), pct_start=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('bounding-box-model', return_path=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement different loss function like detn_l1\n",
    "    # IoU??\n",
    "preds, targs = learn.get_preds(ds_type=DatasetType.Valid)\n",
    "targs = targs.squeeze()  # fastai outputs multiple objects per image but we only have 1\n",
    "# making sure the preds values are within the picture\n",
    "preds = torch.clamp(preds, -1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see what the output looks like\n",
    "if verbose:\n",
    "    print(preds.shape, targs.shape)\n",
    "    print(preds[:2])\n",
    "    print(targs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)\n",
    "n = 10  # look at n samples, must be even\n",
    "idxs = np.random.randint(0,len(data.valid_ds), size=n)\n",
    "_, axes = plt.subplots(nrows=n//2, ncols=2, figsize = (n, n*2))\n",
    "for i, ax in zip(idxs, axes.flat):\n",
    "    img = data.valid_ds[i][0].data  # image resize after data is called else original image size\n",
    "    img_name = Path(data.valid_ds.items[i]).name\n",
    "    img_size = img.shape[1:]\n",
    "    targ, pred = targs[i], preds[i]\n",
    "    if verbose: print(f'target: {targ}, pred: {pred}')\n",
    "    Image(img).show(ax=ax,\n",
    "                    # target is white\n",
    "                     y=ImageBBox.create(*img_size, \n",
    "                                        bboxes=targ.unsqueeze(0),\n",
    "                                        scale=False),\n",
    "                    title=img_name)\n",
    "    # Prediction is red\n",
    "    ImageBBox.create(*img_size, \n",
    "                     bboxes=pred.unsqueeze(0),\n",
    "                     scale=False).show(ax=ax, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the ones that are most way off. \n",
    "## use the custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop image based on pred (not ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_fp = data_fp/\"train-crop-224\"\n",
    "cropped_fp.mkdir(parents=True, exist_ok=True) # save crop images to reduce computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = data_fp/'train_playground'\n",
    "files = get_files(test_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = learn.get_preds(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = iter(learn.data.test_dl.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(learn.data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.pred_batch(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.pred_batch??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_dl.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = learn.data.one_batch(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = learn.data.valid_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.pred_batch(ds_type=DatasetType.Test, batch=([tmp[0].data],[tmp[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = learn.data.valid_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tmp[1]]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageBBox.create(224, 224, bboxes=[[-.5,-.5,.5,.5]], scale=False, labels=[0], classes=['whale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_ds.y = [tmp]* 7960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(learn.data.valid_ds.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(learn.data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.valid_ds.y.new([ImageBBox.create(224, 224, bboxes=[[-.5,-.5,.5,.5]], scale=False, labels=[0], classes=['whale'])]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = learn.get_preds(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = learn.get_preds(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = learn.data.one_batch(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_fp.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image('data\\\\train_playground\\\\.\\\\9c855d38.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.size)\n",
    "print(bbox.data)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bbox.data[0]+1) * torch.tensor([img.size[0]//2,img.size[1]//2]*2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show( y=ImageBBox.create(*img.size, \n",
    "                             bboxes=(bbox.data[0]+1) * torch.tensor([img.size[0]//2,img.size[1]//2]*2).float(), \n",
    "                             scale=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bbox.data[0]+1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1.,2.]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bbox.data[0]+1).squeeze().unsqueeze(1) * torch.tensor([[1.,2.]]).squeeze().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(5,3,4,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(  3,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, bbox = data.valid_ds[89]\n",
    "print(img.size)\n",
    "img.show(y=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_bbox = ((bbox.data[0]+1)*112).int().squeeze().numpy(); crop_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_img = Image(img.data[:, crop_bbox[0]:crop_bbox[2], crop_bbox[1]:crop_bbox[3]])\n",
    "print(crop_img)\n",
    "crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_img.resize(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pad(Image(img.data[:, crop_bbox[0]:crop_bbox[2], crop_bbox[1]:crop_bbox[3]]), 224, 'zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_bbox[0], crop_bbox[2], crop_bbox[1],crop_bbox[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pad(img, 224, 'reflection', row_pct=, col_pct=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pad??  ## resize_method=ResizeMethod.PAD, padding_mode='reflection'  Do not wnat continue to squish the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(img[:, 0:112, 0:112])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data = preds.numpy()).to_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export the model as well so people do not need to retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
